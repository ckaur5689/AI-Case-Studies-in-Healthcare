# AI-Case-Studies-in-Healthcare
Case studies examining AI successes and failures in real-world applications, notably in Healthcare.

# Case Study 1

Real-World life example of application of Machine Learning classification in Healthcare:

# Patient at risk of Urgent and Emergency Care (UEC) Hospitalisation in next 12 months. 
In the real patient level data, it is a relatively small proportion of the total patient population who would be admitted to hospital due via. the UEC route, hence this is a minority class that the model would be predicting. 

The remaining patients (majority class) are relatively healthy, any health conditions they have are being managed well in Primary Care (GP) and would not experience a UEC event in the next 12 months.

Predicting the minority class is meaningful as this allows Acute Hospitals to predict demand of their services and flex capacity but more importantly enables GPs to pro-actively target their practice‚Äôs more at risk patients with providing additional checks, referrals to community based services or advice on lifestyle choices;  thus reducing the risk of hospitalisation due to UEC which can be very disruptive prolonged event in a  patient‚Äôs life. It also puts patients at increase of infections from other hospital patients and is one of the costliest forms of healthcare provision in the NHS. 

This is quite often what Risk Stratification tools used in healthcare are attempting to predict e.g. John Hopkins Risk Stratification Model 
[https://www.hopkinsacg.org/risk-stratification-101-what-is-it-and-how-is-it-used/](https://www.hopkinsacg.org/risk-stratification-101-what-is-it-and-how-is-it-used/)

The imbalance in the classes would mean the ‚ÄúRisk of hospitalisation in the next 12 months‚Äù would be difficult to predict as the model does not have enough data to learn and therefore to predict the minority class. There would be more Type II errors (False negatives) i.e. predicting a patient would not be at risk of hospitalisation (majority class) when they actually are (minority class). This would render the model outputs useless, and GPs would be missing patients they could have targeted for prevention and no robust way of predicting demand into Acute Hospitals.

üìçTherefore, correcting for the imbalance (stratified sampling or oversampling) is essential to carry out first. 

By oversampling the minority class, to correct for the class imbalance, there is more data made available, for the class of interest for the model to learn and test against. However the trade-off is overfitting and the model created might only show good performance on the local patient population that it was trained on. If the model was applied to a different local patient population with for example different social demographics, then there is a risk of the model not being generalisable enough and performance dropping significantly. 

# IBM Watson Oncology Model

Interesting case study which aligns well to the above case of the dangers of an overfitted model; IBM‚Äôs Watson Oncology model was first developed using patient‚Äôs data from Memorial Sloan Kettering to predict the most effective cancer treatment for individual patients for specific cancers. While this hospital was highly regarded for cancer treatment worldwide, some raised concerns that Watson's results may be biased, as they largely drew from a wealthier subset of patients. Page 10 IBM Watson: [https://healthark.ai/wp-content/uploads/2023/11/IBM-Watson-From-healthcare-canary-to-a-failed-prodigy_1.pdf](https://healthark.ai/wp-content/uploads/2023/11/IBM-Watson-From-healthcare-canary-to-a-failed-prodigy_1.pdf)

IBM later used synthetic patient data, but the recommendations of which cancer treatment was based on preferences from a few specialists for each cancer type, rather than using established guidelines or evidence. This introduced biasness in the model, thus introducing another shortcoming of the model.  [IBM's Watson recommended 'unsafe and incorrect' cancer treatments | STAT](https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/)

# Case Study 2

Th Optum algorithm involving an AI / machine learning model used by Optum (a subsidiary of UnitedHealth Group) for population health management‚Äîthat is, to identify which patients should be offered ‚Äúhigh-risk care management‚Äù programs (extra attention, care coordination, home visits, etc

üß† 1. What the algorithm was designed to do

Optum‚Äôs model was a risk-prediction system used by hospitals and insurers to rank patients by future healthcare risk‚Äîessentially:

	‚ÄúWhich patients are likely to need the most care in the future?‚Äù

It assigned a risk score to each patient. Those with the highest scores were enrolled in intensive ‚Äúcare management‚Äù programs aimed at reducing complications and hospitalizations.

According to Optum, this system was used for tens of millions of patients across the U.S., often under product names like Impact Pro or Impact Intelligence (depending on the client).

‚öôÔ∏è 2. The modeling approach (type of ML)

The algorithm was based on supervised machine learning using regression and decision-tree ensembles (typical of healthcare risk-adjustment models).
Specifically:

	‚Ä¢	Type: Predictive modeling / regression (likely a proprietary variant of gradient boosted trees or random forest regression).
	‚Ä¢	Target variable: The predicted healthcare cost (spending) for each patient in the next year.
	‚Ä¢	Inputs (features):
	‚Ä¢	Age, gender, diagnoses (ICD codes)
	‚Ä¢	Lab results and prior utilisation (hospitalizations, ED visits)
	‚Ä¢	Prescriptions and comorbidities
	‚Ä¢	Past healthcare spending
	‚Ä¢	Output: A numeric score estimating expected future cost.

This kind of model is common in actuarial analytics‚Äîinsurers use it to forecast costs, adjust risk pools, and plan care budgets.

However‚Ä¶ this choice of target variable is what led to the problem.

‚ö†Ô∏è 3. The core flaw ‚Äî proxy label bias

The model did not directly predict future medical need or disease severity.
Instead, it predicted future spending, assuming that cost = health need.

That‚Äôs a proxy label, and in healthcare it‚Äôs a dangerous shortcut, because cost is strongly influenced by social and systemic inequities:
	‚Ä¢	Black patients and lower-income patients often spend less on healthcare (due to barriers to access, under-insurance, historical bias, or mistrust).
	‚Ä¢	Therefore, for two equally sick patients‚Äîone white, one Black‚Äîthe model ‚Äúlearned‚Äù that the Black patient would likely generate less cost ‚Üí lower predicted ‚Äúrisk.‚Äù

In other words:

	The model mistook under-utilisation of care for lower medical need.

That meant that Black patients were far less likely to be flagged for high-risk care management‚Äîeven though their actual disease burden was equal or higher.

üìä 4. Quantified impact (from the Science study)

The flaw was discovered by Obermeyer et al., Science (2019):

	‚ÄúDissecting racial bias in an algorithm used to manage the health of populations‚Äù (Science 366, 447‚Äì453).

Findings:
	‚Ä¢	The algorithm‚Äôs bias was systematic and large.
	‚Ä¢	Among patients assigned the same risk score:
	‚Ä¢	Black patients were significantly sicker (had more chronic conditions and higher disease severity).
	‚Ä¢	As a result:
	‚Ä¢	Only 18% of the patients identified for extra care were Black.
	‚Ä¢	If the model had been unbiased, 46% would have been Black.
	‚Ä¢	The bias affected millions of patients nationwide.

üß© 5. Why it happened (technical chain)

Of course ‚Äî here‚Äôs the information from sections 5 and 6 of the Optum algorithm explanation rewritten in UK English bullet-point form.

‚ö†Ô∏è 5. Why the bias happened ‚Äì technical chain

	‚Ä¢	Design choice: the algorithm‚Äôs target variable was future healthcare cost rather than actual medical need.
‚Üí This meant that existing inequalities in healthcare access were baked into the model.
	‚Ä¢	Training data: based on historical insurance claims, which already reflected racial and socio-economic disparities.
	‚Ä¢	Feature correlations: variables linked to spending (such as postcode or previous service use) reinforced the assumption that lower cost equals lower risk.
	‚Ä¢	Deployment context: the risk score was used to select patients for high-risk care management programmes.
‚Üí Result: many Black patients were incorrectly ranked as lower risk.
	‚Ä¢	Validation metric: the model was assessed using cost prediction accuracy (e.g., correlation or RMSE) rather than real-world health outcomes or fairness indicators.

Overall consequence:
A technically sound cost-prediction model produced systematically biased clinical decisions because its design objective was mis-aligned with healthcare equity.

üßÆ 6. The type of machine-learning model

	‚Ä¢	Model type: supervised regression ‚Äì predictive modelling of continuous outcomes.
	‚Ä¢	Algorithm family: ensemble methods such as gradient-boosted trees, random forests, or traditional generalised linear models.
	‚Ä¢	Training data: historical claims records covering millions of patients.
	‚Ä¢	Features (inputs): demographics, diagnoses (ICD codes), lab results, previous hospital and GP visits, prescribed medicines, and total past spending.
	‚Ä¢	Label / target variable: the following year‚Äôs total healthcare cost per patient.
	‚Ä¢	Loss function: mean-squared or mean-absolute error on cost prediction.
	‚Ä¢	Evaluation metric: R¬≤ or RMSE on predicted cost, not on health outcomes.
	‚Ä¢	Deployment output: a numeric risk score used to rank patients by predicted cost (interpreted as risk).
	‚Ä¢	Implementation environment: typical actuarial analytics tools ‚Äì SAS, Python (scikit-learn/XGBoost), or Optum‚Äôs proprietary platforms.

Summary:
The Optum algorithm used a standard supervised regression framework, but by optimising for future cost rather than future illness, it encoded structural bias and misrepresented genuine patient need.

It was likely implemented in SAS, Python (scikit-learn / XGBoost), or proprietary actuarial platforms used by Optum.

üß≠ 7. How it was corrected

After the Science study:
	‚Ä¢	Optum collaborated with researchers and redesigned the model to use direct health outcomes (e.g., number of chronic conditions, hospitalization likelihood) rather than cost.
	‚Ä¢	Adjusted to include social determinants of health and fairness constraints.
	‚Ä¢	Bias reduced dramatically‚Äîroughly an 84% reduction in racial disparity when retrained on need-based labels.

ü©∫ 8. Broader lesson for healthcare AI

This case became the canonical example of ‚Äúproxy-label bias‚Äù in applied machine learning.

Core lesson:

	A model can be ‚Äúaccurate‚Äù on paper but ethically and clinically harmful if its label encodes systemic bias.

So:
	‚Ä¢	Always validate models for equity across subgroups.
	‚Ä¢	Carefully audit what outcome the algorithm truly learns.
	‚Ä¢	Prefer health outcomes or clinical measures of need over economic proxies.
	‚Ä¢	Monitor models post-deployment for drift and fairness.

# How was the Optum algorithm fixed?

ü©∫ How the Optum Algorithm Was Fixed

After the 2019 Science study revealed serious racial bias in Optum‚Äôs risk-prediction model, researchers and Optum‚Äôs own data scientists collaborated to identify and correct the causes. The solutions focused on three main areas: changing the target variable, improving fairness checks, and re-validating the model‚Äôs purpose.

1. Replacing the Target Variable
	‚Ä¢	The original model predicted future healthcare cost as a proxy for medical risk.
	‚Ä¢	This was replaced with a measure of true health need ‚Äì such as:
	‚Ä¢	Number and severity of chronic conditions
	‚Ä¢	Rates of hospitalisation or emergency admissions
	‚Ä¢	Laboratory results or clinical outcome scores
	‚Ä¢	By training the model to predict actual health outcomes rather than cost, it could better identify patients who were genuinely unwell, regardless of how much had been spent on their care.

Effect: This change removed the cost bias that had previously penalised groups with lower healthcare spending, particularly Black patients.

2. Adding Fairness and Subgroup Validation
	‚Ä¢	The revised model was explicitly tested for performance across demographic groups (race, gender, income level, and age).
	‚Ä¢	Analysts compared:
	‚Ä¢	Whether patients with similar clinical need received similar risk scores across groups
	‚Ä¢	Whether enrolment rates in care programmes matched actual disease prevalence
	‚Ä¢	If disparities were detected, the model was adjusted or reweighted to improve equity.

Effect: This step ensured that the algorithm‚Äôs predictions did not systematically favour or disadvantage any subgroup.

3. Redefining the Evaluation Metrics
	‚Ä¢	The previous system evaluated success based on cost-prediction accuracy (e.g., R¬≤ or RMSE on cost).
	‚Ä¢	The updated approach evaluated models using clinical and fairness metrics, including:
	‚Ä¢	Accuracy in predicting hospitalisations or complications
	‚Ä¢	Alignment between predicted and actual health status across patient groups
	‚Ä¢	Bias and disparity indices to track fairness over time

Effect: The model‚Äôs goal shifted from financial efficiency to clinical equity and accuracy.

4. Improved Data Governance and Monitoring
	‚Ä¢	Optum introduced more robust data governance processes to review model inputs, training data, and outcomes.
	‚Ä¢	Regular audits and post-deployment monitoring were added to detect bias drift ‚Äî where a model might gradually become less fair as healthcare patterns change.

Effect: This helps maintain fairness and reliability throughout the model‚Äôs lifecycle, not just at deployment.

‚úÖ Overall Impact

When retrained and revalidated using these methods, the algorithm‚Äôs racial bias was reduced by roughly 80‚Äì85%.
Black patients with equal disease burden were far more likely to be correctly identified as high-risk and offered appropriate care-management support.

In summary:

	Optum‚Äôs algorithm was fixed by moving from cost-based prediction to need-based prediction, embedding fairness testing and equity monitoring into every stage of the model‚Äôs design and deployment.

